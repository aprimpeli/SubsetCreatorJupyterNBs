{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates schema subset files. Works with different threads and each thread writes in a separate file. This speeds up the process a lot in comparison to writing in one global file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from multiprocessing import Pool, Process, Manager\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "subset_class_path = \"\"\n",
    "class_list_file = \"/work-ceph/aprimpel/WDC_Extraction_2020/schemaOrgclasses.txt\"\n",
    "class_list_file_pd = pd.read_csv(class_list_file, sep='\\t', names=['type','filename'])\n",
    "class_list_file_dict = pd.Series(class_list_file_pd.filename.values,index=class_list_file_pd.type).to_dict()\n",
    "input_path = \"/work-ceph/aprimpel/WDC_Extraction_2020/6_sorted_quads/json_ld/\"\n",
    "files_json = [f for f in listdir(input_path) if isfile(join(input_path, f))]\n",
    "\n",
    "output_path = \"/work-ceph/aprimpel/WDC_Extraction_2020/7_c_schema_json_full_no_enc_issues/\"\n",
    "\n",
    "for key in class_list_file_dict:\n",
    "    os.makedirs(output_path+class_list_file_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSubsets(filepath):\n",
    "            \n",
    "    current_url = \"\"\n",
    "    types_of_url = set()\n",
    "    lines_of_url = []\n",
    "    \n",
    "    lines_to_append = dict()\n",
    "    \n",
    "    for schema_class in class_list_file_dict.keys():\n",
    "        lines_to_append[schema_class] = []\n",
    "    \n",
    "    with  gzip.open(input_path+filepath, 'r') as file_:\n",
    "        for line_ in file_:\n",
    "            try:\n",
    "                line = line_.decode('utf8')\n",
    "                url = line.split()[-2]\n",
    "                \n",
    "            except: \n",
    "                print(\"Bad line\")\n",
    "                continue\n",
    "            \n",
    "            if url!= current_url and current_url!=\"\":\n",
    "                #write lines\n",
    "                for type_of_url in types_of_url:\n",
    "                    class_file_name = class_list_file_dict.get(type_of_url)\n",
    "                    lines_to_append[type_of_url].extend(lines_of_url)\n",
    "\n",
    "                types_of_url = set()\n",
    "                lines_of_url = []\n",
    "\n",
    "                \n",
    "            lines_of_url.append(line)\n",
    "            \n",
    "\n",
    "            if (\"<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>\" in line):\n",
    "                for schema_class in class_list_file_dict.keys():\n",
    "                    for prot in ['http','https']:\n",
    "                        if (\"<http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <\"+prot+\"://\"+schema_class+\">\" in line):\n",
    "                            types_of_url.add(schema_class)\n",
    "\n",
    "            \n",
    "        \n",
    "            current_url=url\n",
    "        \n",
    "        #write the last\n",
    "        for type_of_url in types_of_url:\n",
    "            lines_to_append[type_of_url].extend(lines_of_url)\n",
    "    \n",
    "    for key in lines_to_append:\n",
    "        class_file = gzip.open(output_path+class_list_file_dict[key]+\"/\"+class_list_file_dict[key]+\"___\"+filepath+\".gz\", 'a')\n",
    "        for enc_line in lines_to_append[key]:\n",
    "            class_file.write(enc_line.encode())\n",
    "        class_file.close()\n",
    "    \n",
    "    #return lines_to_append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5273/5273 [7:59:17<00:00,  5.45s/it]   \n"
     ]
    }
   ],
   "source": [
    "pool = Pool(60)\n",
    "for result in tqdm(pool.imap(func=createSubsets, iterable=files_json), total=len(files_json)):\n",
    "    pass\n",
    "    '''\n",
    "    for key in result:\n",
    "        class_file = gzip.open(output_path+class_list_file_dict[key]+\".gz\", 'a')\n",
    "        for enc_line in result[key]:\n",
    "            class_file.write(enc_line.encode())\n",
    "        class_file.close()\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
